## GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models
苹果研究者发现：无论是OpenAI GPT-4o和o1系列闭源模型，还是Llama、Phi、Gemma和Mistral等开源模型，都未被发现任何形式推理的证据，而更像是复杂的模式匹配器。

作者把2021年发布的GSM8K数据集中的数学题进行修改得到一个名为GSM-Symbolic的数据集。主要是对题目的人名、数字等进行修改，但题目内核相同，来判断大模型在执行逻辑推理任务时是否会受到无关数据的影响。无论是主流的开源模型还是闭源的GPT系列模型，甚至专门为数理推断专门优化的o1模型，当面对GSM-Symbolic的换皮题目时，准确率都会下降。大多数模型在GSM-Symbolic上的平均性能，都低于在GSM8K上的平均性能。**即使只更改了题目中的名称，大模型的表现也会有存在差异，当只改变了题目中的专有名词时，性能下降在1%-2%之间，当实验者更改数字或结合两类更改时，差异则更为显著。**

此外在题目中添加许多看似相关实际无关的数据得到GSM-NoOp数据集，当模型面对增加了和题目无关的论述的题（GSM-NoOP），性能的下降更是惨不忍睹。这是由于模型会将无关的论述当成需要操作的步骤，从而画蛇添足地回答错误。

结论：大模型实际不是解数学题，还是在进行模式匹配

## Case-based or rule-based: How Do Transformers Do the Math? 

人类可以轻松地学习加法的基本规则，例如竖式加法，并将其应用于任意长度的新的加法问题，但 LLMs 却难以做到这一点。相反，它们可能会依赖于训练语料库中见过的相似样例来帮助解决问题。张牧涵团队的 ICML 2024 论文深刻研究了这一现象。研究者们将这两种不同的推理机制定义为 “基于规则的推理”（rule-based reasoning）和 “基于样例的推理”（case-based reasoning）。

为了测试模型是否依赖特定样例来解决问题，作者使用了 Leave-Square-Out 方法。主要思想是首先需要定位模型可能依赖的训练集中的样例，然后将它们从训练集中移除，以观察它们是否影响模型的测试性能。对于数学推理，作者的假设是，在解决某个测试样本时，transformers 倾向于依赖与测试样本 “接近” 的训练样本来进行推理。因此，作者在样本的二维空间中挖掉了一块正方形作为测试集（test square）。根据假设，若模型在做 case-based reasoning，且模型依赖的是与 test sample 距离较近的 training sample 来做推理，那么模型将无法答对正方形中心附近的 test samples，因为模型在训练集中没有见过接近的样例。

![](figure\f_1.jpg)

结论：把 holes 周围的 similar cases 移出训练集时，模型便无法做对 holes 中的 test samples 做出准确推理。也即展现出模型依赖 similar cases 进行推理的行为。

改进：
    1. 通过加入 scratchpad，即引导模型在输出中一位一位地做加法来消除 case-based reasoning 的行为，使模型转向 rule-based reasoning，效果不佳
    2. 提出了名为 Rule-Following Fine-Tuning（RFFT）的规则遵循微调技术，具体来说，RFFT 在输入中提供显式的规则，然后指导 transformers 逐行地回忆规则并执行。效果较好

具体方法如图
![](figure\f_2.jpg)

RFFT 在长度泛化的性能上明显超过了 direct answer 和 scratchpad 这两种微调方法。对于GPT-3.5-turbo，RFFT 使其能够惊人地泛化到涉及多达 12 位数字的加法，尽管只在 1-5 位加法上训练了 100 个训练样本，但其在 12 位数的加法上仍然保持了 95% 以上的准确率。

此外，作者发现 Llama-2-7B 需要 150,000 个训练样本才能泛化到 9 位数字，而 GPT-3.5 仅用 100 个训练样本就能掌握规则并泛化到 12 位数字。因此，规则遵循（rule-following）可能是一种 meta learning ability—— 它可能通过在多样化的 rule-following 数据上进行训练而得到加强，并可更容易地迁移到新的未在训练集中见过的领域中。相应地，基础模型越强大，理解并学习新的规则就越容易。这也与人类学习新规则的能力相符 —— 经验丰富的学习者通常学习得更快。


